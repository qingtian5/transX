{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0875ee0-48c8-4689-a5ff-2cf9fc6a7354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @description:\n",
    "# @author: zchen\n",
    "# @time: 2020/12/8 21:09\n",
    "# @file: preprocessor.py.py\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from config import Config\n",
    "from logger import logger\n",
    "from utils import load_file\n",
    "\n",
    "\n",
    "class TransXProcessor(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_data(config: Config):\n",
    "        \"\"\"\n",
    "        根据数据集生成entity2id,relation2id文件，此操作可以通过其他方式实现，当数据量过大时，可以采用hadoop进行处理\n",
    "        :param config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        data_path = []\n",
    "        if config.do_train: #确定是否需要训练和评估数据\n",
    "            data_path.append(config.train_data_path)\n",
    "\n",
    "        if config.do_eval:\n",
    "            data_path.append(config.eval_data_path)\n",
    "\n",
    "        #将所有数据集合并成一个数据框 raw_df\n",
    "        raw_df = pd.concat([pd.read_csv(p,\n",
    "                                        sep=\"\\t\",\n",
    "                                        header=None,\n",
    "                                        names=[\"head\", \"relation\", \"tail\"],\n",
    "                                        keep_default_na=False,\n",
    "                                        encoding=\"utf-8\") for p in data_path], axis=0) #沿着行方向拼接\n",
    "        raw_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #统计头实体、尾实体和关系的出现次数\n",
    "        head_counter = Counter(raw_df[\"head\"])\n",
    "        tail_counter = Counter(raw_df[\"tail\"])\n",
    "        relation_counter = Counter(raw_df[\"relation\"])\n",
    "\n",
    "        # Generate entity and relation list\n",
    "        entity_list = list((head_counter + tail_counter).keys())\n",
    "        relation_list = list(relation_counter.keys())\n",
    "\n",
    "        # Transform to index dict and save\n",
    "        #每个索引字典中的元素格式为 [word, index]，表示实体或关系对应的名称和索引\n",
    "        entity_index = [[word, index] for index, word in enumerate(entity_list)]\n",
    "        relation_index = [[word, index] for index, word in enumerate(relation_list)]\n",
    "\n",
    "        pd.DataFrame(entity_index).to_csv(config.entity2id_path, sep=\"\\t\", header=False, index=False, encoding=\"utf-8\")\n",
    "        pd.DataFrame(relation_index).to_csv(config.relation2id_path, sep=\"\\t\", header=False, index=False,\n",
    "                                            encoding=\"utf-8\")\n",
    "\n",
    "    def init_data_dict(self, config: Config):\n",
    "        \"\"\"\n",
    "        初始化实体和实体ID、关系和关系ID\n",
    "        :param config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 用来存放实体和实体ID、关系和关系ID\n",
    "        # 格式: {关系:关系ID}、{关系:关系ID}\n",
    "        entity_dict, entity_total = self._read_ent_rel_data(config.entity2id_path)\n",
    "        relation_dict, relation_total = self._read_ent_rel_data(config.relation2id_path)\n",
    "        return entity_dict, relation_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_ent_rel_data(path):\n",
    "        content_dict = dict()\n",
    "        content_len = 0\n",
    "        try:\n",
    "            lines = load_file(path)\n",
    "            for i in lines:\n",
    "                content_dict[i.strip().split('\\t')[0]] = int(i.strip().split('\\t')[1])\n",
    "            content_len = len(lines)\n",
    "        except IOError:\n",
    "            logger.error(\"Error: 没有找到文件或读取文件失败\", path)\n",
    "            exit(-1)\n",
    "        return content_dict, content_len\n",
    "\n",
    "    @staticmethod\n",
    "    def data_set(config: Config, entity_dict, relation_dict, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        获取指定数据集，将其转换成ID\n",
    "        :param config:\n",
    "        :param entity_dict:\n",
    "        :param relation_dict:\n",
    "        :param mode:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 用来存放三元组数据,格式:头实体\\t关系\\t尾实体\n",
    "        triple_lists = list()\n",
    "        triple_path = config.train_data_path if mode == \"train\" else config.eval_data_path\n",
    "\n",
    "        def word2id(word):\n",
    "            try:\n",
    "                return int(entity_dict[word])\n",
    "            except KeyError:\n",
    "                return int(relation_dict[word])\n",
    "\n",
    "        try:\n",
    "            lines = load_file(triple_path)\n",
    "            for i in lines:\n",
    "                triple_list = i.split('\\t')\n",
    "                head, relation, tail = triple_list[0].strip(), triple_list[1].strip(), triple_list[2].strip()\n",
    "                triple_lists.append(\n",
    "                    str(word2id(head)) + \"_\" + str(word2id(relation)) + \"_\" + str(word2id(tail)))\n",
    "        except IOError:\n",
    "            logger.error('文件夹内没有triple2id.txt，\\ntriple2id.txt文件每一行是:开始实体\\\\t结束实体\\\\t关系')\n",
    "            exit(-1)\n",
    "\n",
    "        return triple_lists\n",
    "\n",
    "    @staticmethod\n",
    "    def data_loader(config, data_set, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        获取实体、关系、data_loader\n",
    "        :param config:\n",
    "        :param data_set:\n",
    "        :param mode:一个字符串参数，指示加载器是用于训练还是评估。默认为 \"train\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert (mode in [\"train\", \"eval\"])\n",
    "        tds = TransXDataSet(data_set=data_set) #用于处理数据集\n",
    "        if mode == \"train\":\n",
    "            # 生成负样本 -调用 tds 对象的 generate_neg_sampler 方法，生成负样本\n",
    "            tds.generate_neg_sampler(rep_prob=config.rep_proba, ex_prob=config.ex_proba)\n",
    "            loader = DataLoader(tds, batch_size=config.batch_size, shuffle=True, drop_last=False) #批量加载数据并打乱\n",
    "        else:\n",
    "            loader = DataLoader(tds, batch_size=config.batch_size, shuffle=False, drop_last=False)\n",
    "        return loader\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_output(config: Config):\n",
    "        \"\"\"\n",
    "        清理output目录，若output目录存在，将会被删除, 然后初始化输出目录\n",
    "        :param config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if config.do_train:\n",
    "            logger.info(f\"check up output dir and clear dir: {config.output_path}\")\n",
    "            if os.path.exists(config.output_path):\n",
    "                def del_file(path):\n",
    "                    ls = os.listdir(path)\n",
    "                    for i in ls:\n",
    "                        c_path = os.path.join(path, i)\n",
    "                        if os.path.isdir(c_path):\n",
    "                            del_file(c_path)\n",
    "                            os.rmdir(c_path)\n",
    "                        else:\n",
    "                            os.remove(c_path)\n",
    "\n",
    "                try:\n",
    "                    del_file(config.output_path)\n",
    "                except Exception as e:\n",
    "                    logger.error(e)\n",
    "                    logger.error('pleace remove the files of output dir and data.conf')\n",
    "                    exit(-1)\n",
    "\n",
    "        # 初始化output目录\n",
    "        if os.path.exists(config.output_path) and os.listdir(config.output_path) and config.do_train:\n",
    "            raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(config.output_path))\n",
    "\n",
    "        if not os.path.exists(config.output_path):\n",
    "            os.makedirs(config.output_path)\n",
    "\n",
    "        if not os.path.exists(config.dump_embedding_path):\n",
    "            os.makedirs(config.dump_embedding_path)\n",
    "\n",
    "        if not os.path.exists(config.model_path):\n",
    "            os.makedirs(config.model_path)\n",
    "\n",
    "\n",
    "class TransXDataSet(Dataset):\n",
    "    def __init__(self, data_set):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.df = self._list2data_frame(data_set=data_set)\n",
    "\n",
    "    @staticmethod\n",
    "    def _list2data_frame(data_set):\n",
    "        res = [i.split(\"_\") for i in data_set]\n",
    "        df = pd.DataFrame(res, columns=[\"head\", \"relation\", \"tail\"])\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if hasattr(self, \"neg_df\"):\n",
    "            #hasattr(self, \"neg_df\") 用于检查当前对象是否有 neg_df 属性。如果有，说明这个数据集对象包含了负样本数据\n",
    "            return np.array(self.df.iloc[item, :3], dtype=np.int64), np.array(self.neg_df.iloc[item, :3],\n",
    "                                                                              dtype=np.int64)\n",
    "        else:\n",
    "            return np.array(self.df.iloc[item, :3], dtype=np.int64)\n",
    "\n",
    "    def generate_neg_sampler(self, rep_prob=0.5, ex_prob=0.5):\n",
    "        \"\"\"\n",
    "        假设将所有输入正例三元组看成是shape(N, 3)的矩阵，按照论文的要求，要么替换head，要么替换tail.\n",
    "\n",
    "        shuffle_head：将矩阵第一列使用random.shuffle()得到的混排的head(相当于论文里面所说的随机找一个替换head的实体)\n",
    "        shuffle_tail：将矩阵第三列使用random.shuffle()得到的混排的tail(相当于论文里面所说的随机找一个替换tail的实体)\n",
    "        rep_prob_distribution：使用random.random((0, 1))生成长度为N的随机数列表\n",
    "        ex_prob_distribution：使用random.random((0, 1))生成长度为N的随机数列表\n",
    "\n",
    "        那么可以设定一个阈值rep_proba，如果rep_prob_distribution的值小于rep_proba，那么使用shuffle_head对应位置的实体替换原来的头实体，\n",
    "        反之，使用shuffle_tail对应位置的实体替换原来的尾实体。\n",
    "\n",
    "        可以看出，当N足够大时，产生出来的负样本与正样本碰撞的概率几乎为零，基本符合论文要求。\n",
    "        :param rep_prob: (float)Probability of replacing head\n",
    "        :param ex_prob:  (float)Probability of replacing head with tail entities or replacing tail with head entities.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.neg_df = self.df.copy()\n",
    "\n",
    "        shuffle_head = self.neg_df[\"head\"].sample(frac=1.0, random_state=0) #random_state=0每次调用 sample 方法时都会生成相同的随机结果\n",
    "        shuffle_tail = self.neg_df[\"tail\"].sample(frac=1.0, random_state=0)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        rep_prob_distribution = np.random.uniform(low=0.0, high=1.0, size=(len(self.neg_df),))\n",
    "\n",
    "        np.random.seed(0)\n",
    "        ex_prob_distribution = np.random.uniform(low=0.0, high=1.0, size=(len(self.neg_df),))\n",
    "\n",
    "        # Replacing head or tail\n",
    "        def replace_head(rel_head, shuffle_head, shuffle_tail, rep_p, ex_p):\n",
    "            if rep_p >= rep_prob:\n",
    "                '''\n",
    "                Not replacing head.self.negD\n",
    "                '''\n",
    "                return rel_head\n",
    "            else:\n",
    "                if ex_p > ex_prob:\n",
    "                    '''\n",
    "                    Replacing head with shuffle head.\n",
    "                    '''\n",
    "                    return shuffle_head\n",
    "                else:\n",
    "                    '''\n",
    "                    Replacing head with shuffle tail.\n",
    "                    '''\n",
    "                    return shuffle_tail\n",
    "\n",
    "        def replace_tail(rel_tail, shuffle_head, shuffle_tail, rep_p, ex_p):\n",
    "            if rep_p < rep_prob:\n",
    "                '''\n",
    "                Not replacing tail.\n",
    "                '''\n",
    "                return rel_tail\n",
    "            else:\n",
    "                if ex_p > ex_prob:\n",
    "                    '''\n",
    "                    Replacing tail with shuffle tail.\n",
    "                    '''\n",
    "                    return shuffle_tail\n",
    "                else:\n",
    "                    '''\n",
    "                    Replacing head with shuffle head.\n",
    "                    '''\n",
    "                    return shuffle_head\n",
    "\n",
    "        self.neg_df[\"head\"] = list(\n",
    "            map(replace_head, self.neg_df[\"head\"], shuffle_head, shuffle_tail, rep_prob_distribution,\n",
    "                ex_prob_distribution))\n",
    "        self.neg_df[\"tail\"] = list(\n",
    "            map(replace_tail, self.neg_df[\"tail\"], shuffle_head, shuffle_tail, rep_prob_distribution,\n",
    "                ex_prob_distribution))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TransXProcessor().generate_data(config=Config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3829e5-ab0f-4e20-95a7-6d84ac53d6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa863b-e798-44bd-9673-b1bce9b26dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
