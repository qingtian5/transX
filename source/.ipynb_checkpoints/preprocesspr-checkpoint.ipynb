{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0875ee0-48c8-4689-a5ff-2cf9fc6a7354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/autodl-tmp/transX/data/valid_test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 278\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneg_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    273\u001b[0m             \u001b[38;5;28mmap\u001b[39m(replace_tail, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneg_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle_head, shuffle_tail, rep_prob_distribution,\n\u001b[1;32m    274\u001b[0m                 ex_prob_distribution))\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[43mTransXProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mTransXProcessor.generate_data\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     33\u001b[0m     data_path\u001b[38;5;241m.\u001b[39mappend(config\u001b[38;5;241m.\u001b[39meval_data_path)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#将所有数据集合并成一个数据框 raw_df\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m raw_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mread_csv(p,\n\u001b[1;32m     37\u001b[0m                                 sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m                                 header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m                                 names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     40\u001b[0m                                 keep_default_na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m                                 encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m data_path], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#沿着行方向拼接\u001b[39;00m\n\u001b[1;32m     42\u001b[0m raw_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#统计头实体、尾实体和关系的出现次数\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     data_path\u001b[38;5;241m.\u001b[39mappend(config\u001b[38;5;241m.\u001b[39meval_data_path)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#将所有数据集合并成一个数据框 raw_df\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m raw_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtail\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m data_path], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#沿着行方向拼接\u001b[39;00m\n\u001b[1;32m     42\u001b[0m raw_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#统计头实体、尾实体和关系的出现次数\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/autodl-tmp/transX/data/valid_test.txt'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @description:\n",
    "# @author: zchen\n",
    "# @time: 2020/12/8 21:09\n",
    "# @file: preprocessor.py.py\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from config import Config\n",
    "from logger import logger\n",
    "from utils import load_file\n",
    "\n",
    "\n",
    "class TransXProcessor(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_data(config: Config):\n",
    "        \"\"\"\n",
    "        根据数据集生成entity2id,relation2id文件，此操作可以通过其他方式实现，当数据量过大时，可以采用hadoop进行处理\n",
    "        :param config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        data_path = []\n",
    "        if config.do_train: #确定是否需要训练和评估数据\n",
    "            data_path.append(config.train_data_path)\n",
    "\n",
    "        if config.do_eval:\n",
    "            data_path.append(config.eval_data_path)\n",
    "\n",
    "        #将所有数据集合并成一个数据框 raw_df\n",
    "        raw_df = pd.concat([pd.read_csv(p,\n",
    "                                        sep=\"\\t\",\n",
    "                                        header=None,\n",
    "                                        names=[\"head\", \"relation\", \"tail\"],\n",
    "                                        keep_default_na=False,\n",
    "                                        encoding=\"utf-8\") for p in data_path], axis=0) #沿着行方向拼接\n",
    "        raw_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #统计头实体、尾实体和关系的出现次数\n",
    "        head_counter = Counter(raw_df[\"head\"])\n",
    "        tail_counter = Counter(raw_df[\"tail\"])\n",
    "        relation_counter = Counter(raw_df[\"relation\"])\n",
    "\n",
    "        # Generate entity and relation list\n",
    "        entity_list = list((head_counter + tail_counter).keys())\n",
    "        relation_list = list(relation_counter.keys())\n",
    "\n",
    "        # Transform to index dict and save\n",
    "        #每个索引字典中的元素格式为 [word, index]，表示实体或关系对应的名称和索引\n",
    "        entity_index = [[word, index] for index, word in enumerate(entity_list)]\n",
    "        relation_index = [[word, index] for index, word in enumerate(relation_list)]\n",
    "\n",
    "        pd.DataFrame(entity_index).to_csv(config.entity2id_path, sep=\"\\t\", header=False, index=False, encoding=\"utf-8\")\n",
    "        pd.DataFrame(relation_index).to_csv(config.relation2id_path, sep=\"\\t\", header=False, index=False,\n",
    "                                            encoding=\"utf-8\")\n",
    "\n",
    "    def init_data_dict(self, config: Config):\n",
    "        \"\"\"\n",
    "        初始化实体和实体ID、关系和关系ID\n",
    "        :param config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 用来存放实体和实体ID、关系和关系ID\n",
    "        # 格式: {关系:关系ID}、{关系:关系ID}\n",
    "        entity_dict, entity_total = self._read_ent_rel_data(config.entity2id_path)\n",
    "        relation_dict, relation_total = self._read_ent_rel_data(config.relation2id_path)\n",
    "        return entity_dict, relation_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_ent_rel_data(path):\n",
    "        content_dict = dict()\n",
    "        content_len = 0\n",
    "        try:\n",
    "            lines = load_file(path)\n",
    "            for i in lines:\n",
    "                content_dict[i.strip().split('\\t')[0]] = int(i.strip().split('\\t')[1])\n",
    "            content_len = len(lines)\n",
    "        except IOError:\n",
    "            logger.error(\"Error: 没有找到文件或读取文件失败\", path)\n",
    "            exit(-1)\n",
    "        return content_dict, content_len\n",
    "\n",
    "    @staticmethod\n",
    "    def data_set(config: Config, entity_dict, relation_dict, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        获取指定数据集，将其转换成ID\n",
    "        :param config:\n",
    "        :param entity_dict:\n",
    "        :param relation_dict:\n",
    "        :param mode:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 用来存放三元组数据,格式:头实体\\t关系\\t尾实体\n",
    "        triple_lists = list()\n",
    "        triple_path = config.train_data_path if mode == \"train\" else config.eval_data_path\n",
    "\n",
    "        def word2id(word):\n",
    "            try:\n",
    "                return int(entity_dict[word])\n",
    "            except KeyError:\n",
    "                return int(relation_dict[word])\n",
    "\n",
    "        try:\n",
    "            lines = load_file(triple_path)\n",
    "            for i in lines:\n",
    "                triple_list = i.split('\\t')\n",
    "                head, relation, tail = triple_list[0].strip(), triple_list[1].strip(), triple_list[2].strip()\n",
    "                triple_lists.append(\n",
    "                    str(word2id(head)) + \"_\" + str(word2id(relation)) + \"_\" + str(word2id(tail)))\n",
    "        except IOError:\n",
    "            logger.error('文件夹内没有triple2id.txt，\\ntriple2id.txt文件每一行是:开始实体\\\\t结束实体\\\\t关系')\n",
    "            exit(-1)\n",
    "\n",
    "        return triple_lists\n",
    "\n",
    "    @staticmethod\n",
    "    def data_loader(config, data_set, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        获取实体、关系、data_loader\n",
    "        :param config:\n",
    "        :param data_set:\n",
    "        :param mode:一个字符串参数，指示加载器是用于训练还是评估。默认为 \"train\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert (mode in [\"train\", \"eval\"])\n",
    "        tds = TransXDataSet(data_set=data_set) #用于处理数据集\n",
    "        if mode == \"train\":\n",
    "            # 生成负样本 -调用 tds 对象的 generate_neg_sampler 方法，生成负样本\n",
    "            tds.generate_neg_sampler(rep_prob=config.rep_proba, ex_prob=config.ex_proba)\n",
    "            loader = DataLoader(tds, batch_size=config.batch_size, shuffle=True, drop_last=False) #批量加载数据并打乱\n",
    "        else:\n",
    "            loader = DataLoader(tds, batch_size=config.batch_size, shuffle=False, drop_last=False)\n",
    "        return loader\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_output(config: Config):\n",
    "        \"\"\"\n",
    "        清理output目录，若output目录存在，将会被删除, 然后初始化输出目录\n",
    "        :param config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if config.do_train:\n",
    "            logger.info(f\"check up output dir and clear dir: {config.output_path}\")\n",
    "            if os.path.exists(config.output_path):\n",
    "                def del_file(path):\n",
    "                    ls = os.listdir(path)\n",
    "                    for i in ls:\n",
    "                        c_path = os.path.join(path, i)\n",
    "                        if os.path.isdir(c_path):\n",
    "                            del_file(c_path)\n",
    "                            os.rmdir(c_path)\n",
    "                        else:\n",
    "                            os.remove(c_path)\n",
    "\n",
    "                try:\n",
    "                    del_file(config.output_path)\n",
    "                except Exception as e:\n",
    "                    logger.error(e)\n",
    "                    logger.error('pleace remove the files of output dir and data.conf')\n",
    "                    exit(-1)\n",
    "\n",
    "        # 初始化output目录\n",
    "        if os.path.exists(config.output_path) and os.listdir(config.output_path) and config.do_train:\n",
    "            raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(config.output_path))\n",
    "\n",
    "        if not os.path.exists(config.output_path):\n",
    "            os.makedirs(config.output_path)\n",
    "\n",
    "        if not os.path.exists(config.dump_embedding_path):\n",
    "            os.makedirs(config.dump_embedding_path)\n",
    "\n",
    "        if not os.path.exists(config.model_path):\n",
    "            os.makedirs(config.model_path)\n",
    "\n",
    "\n",
    "class TransXDataSet(Dataset):\n",
    "    def __init__(self, data_set):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.df = self._list2data_frame(data_set=data_set)\n",
    "\n",
    "    @staticmethod\n",
    "    def _list2data_frame(data_set):\n",
    "        res = [i.split(\"_\") for i in data_set]\n",
    "        df = pd.DataFrame(res, columns=[\"head\", \"relation\", \"tail\"])\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if hasattr(self, \"neg_df\"):\n",
    "            #hasattr(self, \"neg_df\") 用于检查当前对象是否有 neg_df 属性。如果有，说明这个数据集对象包含了负样本数据\n",
    "            return np.array(self.df.iloc[item, :3], dtype=np.int64), np.array(self.neg_df.iloc[item, :3],\n",
    "                                                                              dtype=np.int64)\n",
    "        else:\n",
    "            return np.array(self.df.iloc[item, :3], dtype=np.int64)\n",
    "\n",
    "    def generate_neg_sampler(self, rep_prob=0.5, ex_prob=0.5):\n",
    "        \"\"\"\n",
    "        假设将所有输入正例三元组看成是shape(N, 3)的矩阵，按照论文的要求，要么替换head，要么替换tail.\n",
    "\n",
    "        shuffle_head：将矩阵第一列使用random.shuffle()得到的混排的head(相当于论文里面所说的随机找一个替换head的实体)\n",
    "        shuffle_tail：将矩阵第三列使用random.shuffle()得到的混排的tail(相当于论文里面所说的随机找一个替换tail的实体)\n",
    "        rep_prob_distribution：使用random.random((0, 1))生成长度为N的随机数列表\n",
    "        ex_prob_distribution：使用random.random((0, 1))生成长度为N的随机数列表\n",
    "\n",
    "        那么可以设定一个阈值rep_proba，如果rep_prob_distribution的值小于rep_proba，那么使用shuffle_head对应位置的实体替换原来的头实体，\n",
    "        反之，使用shuffle_tail对应位置的实体替换原来的尾实体。\n",
    "\n",
    "        可以看出，当N足够大时，产生出来的负样本与正样本碰撞的概率几乎为零，基本符合论文要求。\n",
    "        :param rep_prob: (float)Probability of replacing head\n",
    "        :param ex_prob:  (float)Probability of replacing head with tail entities or replacing tail with head entities.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.neg_df = self.df.copy()\n",
    "\n",
    "        shuffle_head = self.neg_df[\"head\"].sample(frac=1.0, random_state=0) #random_state=0每次调用 sample 方法时都会生成相同的随机结果\n",
    "        shuffle_tail = self.neg_df[\"tail\"].sample(frac=1.0, random_state=0)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        rep_prob_distribution = np.random.uniform(low=0.0, high=1.0, size=(len(self.neg_df),))\n",
    "\n",
    "        np.random.seed(0)\n",
    "        ex_prob_distribution = np.random.uniform(low=0.0, high=1.0, size=(len(self.neg_df),))\n",
    "\n",
    "        # Replacing head or tail\n",
    "        def replace_head(rel_head, shuffle_head, shuffle_tail, rep_p, ex_p):\n",
    "            if rep_p >= rep_prob:\n",
    "                '''\n",
    "                Not replacing head.self.negD\n",
    "                '''\n",
    "                return rel_head\n",
    "            else:\n",
    "                if ex_p > ex_prob:\n",
    "                    '''\n",
    "                    Replacing head with shuffle head.\n",
    "                    '''\n",
    "                    return shuffle_head\n",
    "                else:\n",
    "                    '''\n",
    "                    Replacing head with shuffle tail.\n",
    "                    '''\n",
    "                    return shuffle_tail\n",
    "\n",
    "        def replace_tail(rel_tail, shuffle_head, shuffle_tail, rep_p, ex_p):\n",
    "            if rep_p < rep_prob:\n",
    "                '''\n",
    "                Not replacing tail.\n",
    "                '''\n",
    "                return rel_tail\n",
    "            else:\n",
    "                if ex_p > ex_prob:\n",
    "                    '''\n",
    "                    Replacing tail with shuffle tail.\n",
    "                    '''\n",
    "                    return shuffle_tail\n",
    "                else:\n",
    "                    '''\n",
    "                    Replacing head with shuffle head.\n",
    "                    '''\n",
    "                    return shuffle_head\n",
    "\n",
    "        self.neg_df[\"head\"] = list(\n",
    "            map(replace_head, self.neg_df[\"head\"], shuffle_head, shuffle_tail, rep_prob_distribution,\n",
    "                ex_prob_distribution))\n",
    "        self.neg_df[\"tail\"] = list(\n",
    "            map(replace_tail, self.neg_df[\"tail\"], shuffle_head, shuffle_tail, rep_prob_distribution,\n",
    "                ex_prob_distribution))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TransXProcessor().generate_data(config=Config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3829e5-ab0f-4e20-95a7-6d84ac53d6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa863b-e798-44bd-9673-b1bce9b26dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
